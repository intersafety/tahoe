{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b431e80a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'content', 'user_id', 'is_dangerous', 'positive_ratings',\n",
      "       'negative_ratings', 'created_at', 'updated_at', 'scenarios_violation',\n",
      "       'tools_violation', 'intent_violation', 'scenarios_violation_count',\n",
      "       'tools_violation_count', 'intent_violation_count', 'label',\n",
      "       'embedding'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"prompts_rows.csv\")\n",
    "\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9bc6a2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.optim import AdamW\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n",
    "\n",
    "# --- Dataset Definition ---\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        encoding = self.tokenizer(\n",
    "            self.texts[idx],\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        item = {key: val.squeeze(0) for key, val in encoding.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        item['text'] = self.texts[idx]  # Add this line\n",
    "        return item\n",
    "\n",
    "# --- Model Definition ---\n",
    "\n",
    "class ContrastiveModel(nn.Module):\n",
    "    def __init__(self, model_name='bert-base-uncased', projection_dim=128):\n",
    "        super().__init__()\n",
    "        self.encoder = SentenceTransformer(\"../intersafety/intersafety-backend/InterSafetyDeploy/src/local_model\")\n",
    "        hidden_size = 384\n",
    "        self.projector = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, projection_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, texts):\n",
    "        embeddings = self.encoder.encode(texts, convert_to_tensor=True)\n",
    "        embeddings = embeddings.clone()  # <-- Add this line\n",
    "        projected = self.projector(embeddings)\n",
    "        return projected\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, contrastive_model, projection_dim=128, num_classes=2):\n",
    "        super().__init__()\n",
    "        self.encoder = contrastive_model.encoder\n",
    "        self.projector = contrastive_model.projector\n",
    "        self.classifier = nn.Linear(projection_dim, num_classes)\n",
    "\n",
    "    def forward(self, texts):\n",
    "        with torch.no_grad():\n",
    "            embeddings = self.encoder.encode(texts, convert_to_tensor=True)\n",
    "            embeddings = self.projector(embeddings)\n",
    "        logits = self.classifier(embeddings)\n",
    "        return logits\n",
    "\n",
    "# --- Loss Function ---\n",
    "\n",
    "def supervised_contrastive_loss(embeddings, labels, temperature=0.1):\n",
    "    embeddings = F.normalize(embeddings, dim=1)\n",
    "    similarity_matrix = torch.matmul(embeddings, embeddings.T) / temperature\n",
    "    labels = labels.unsqueeze(1)\n",
    "    mask = torch.eq(labels, labels.T).float()\n",
    "    logits_mask = torch.ones_like(mask) - torch.eye(mask.size(0)).to(mask.device)\n",
    "    mask = mask * logits_mask\n",
    "\n",
    "    exp_logits = torch.exp(similarity_matrix) * logits_mask\n",
    "    log_prob = similarity_matrix - torch.log(exp_logits.sum(1, keepdim=True) + 1e-9)\n",
    "\n",
    "    mean_log_prob_pos = (mask * log_prob).sum(1) / (mask.sum(1) + 1e-9)\n",
    "    loss = -mean_log_prob_pos.mean()\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d0b72f65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/porkboi/Documents/contrastive test/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Contrastive Loss: 3.4275\n",
      "Epoch 2 Contrastive Loss: 3.4074\n",
      "Epoch 3 Contrastive Loss: 3.4191\n",
      "Epoch 1 Classifier Loss: 0.6795\n",
      "Epoch 2 Classifier Loss: 0.5960\n",
      "Epoch 3 Classifier Loss: 0.5281\n",
      "Validation Accuracy: 0.9529\n",
      "Model saved as new_encoder.pt\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"prompts_rows.csv\")  # Your CSV path here\n",
    "label_encoder = LabelEncoder()\n",
    "df['label_id'] = label_encoder.fit_transform(df['label'])  # SAFE=0, DANGEROUS=1\n",
    "\n",
    "texts = df['content'].tolist()\n",
    "labels = df['label_id'].tolist()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "dataset = TextDataset(texts, labels, tokenizer)\n",
    "\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = ContrastiveModel().to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "epochs = 3\n",
    "\n",
    "\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        texts_batch = batch['text']\n",
    "        labels_batch = batch['labels'].to(device)\n",
    "\n",
    "        embeddings = model(texts_batch)\n",
    "        loss = supervised_contrastive_loss(embeddings, labels_batch)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1} Contrastive Loss: {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "\n",
    "model.eval()\n",
    "classifier = Classifier(model).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_cls = AdamW(classifier.classifier.parameters(), lr=1e-3)\n",
    "\n",
    "# --- Train Classification Head ---\n",
    "\n",
    "classifier.train()\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        texts_batch = batch['text']\n",
    "        labels_batch = batch['labels'].to(device)\n",
    "\n",
    "        logits = classifier(texts_batch)\n",
    "        loss = criterion(logits, labels_batch)\n",
    "\n",
    "        optimizer_cls.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer_cls.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1} Classifier Loss: {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "# --- Evaluation ---\n",
    "\n",
    "classifier.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in val_loader:\n",
    "        texts_batch = batch['text']\n",
    "        labels_batch = batch['labels'].to(device)\n",
    "\n",
    "        logits = classifier(texts_batch)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels_batch.cpu().numpy())\n",
    "\n",
    "acc = accuracy_score(all_labels, all_preds)\n",
    "print(f\"Validation Accuracy: {acc:.4f}\")\n",
    "\n",
    "torch.save(model.state_dict(), \"new_encoder.pt\")\n",
    "print(\"Model saved as new_encoder.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "56695599",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n"
     ]
    }
   ],
   "source": [
    "# Load your model\n",
    "model = ContrastiveModel()\n",
    "model.load_state_dict(torch.load(\"new_encoder.pt\", map_location=torch.device(\"cpu\")))\n",
    "model.eval()\n",
    "\n",
    "# Input text (can be a single string or list of strings)\n",
    "text = \"This is a test sentence to re-encode.\"\n",
    "projected_embedding = model([text])  # Wrap in list\n",
    "print(len(projected_embedding.tolist()[0]))  # Should be [1, projection_dim]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
